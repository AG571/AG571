name: Daily heartbeat

on:
  workflow_dispatch:
  schedule:
    - cron: '15 07 * * *'  # daily 07:15 UTC

permissions:
  contents: write

jobs:
  hb:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Build AI news summary and update README
        run: |
          python - << 'PY'
          import requests, re, datetime, pathlib

          README_PATH = pathlib.Path("README.md")
          START = "<!--START_HEARTBEAT-->"
          END = "<!--END_HEARTBEAT-->"

          # ---------- Data sources ----------

          def fetch_hn(limit: int = 4):
              """Fetch AI-related stories from Hacker News."""
              url = "https://hn.algolia.com/api/v1/search"
              params = {"query": "AI", "tags": "story", "hitsPerPage": limit}
              try:
                  r = requests.get(url, params=params, timeout=10)
                  r.raise_for_status()
                  data = r.json()
              except Exception as e:
                  print(f"[WARN] Hacker News fetch failed: {e}")
                  return []

              items = []
              for hit in data.get("hits", []):
                  title = hit.get("title")
                  url = hit.get("url") or f"https://news.ycombinator.com/item?id={hit.get('objectID')}"
                  if title and url:
                      items.append({
                          "title": title,
                          "url": url,
                          "source": "Hacker News",
                      })
              return items

          def fetch_reddit(limit: int = 4):
              """Fetch top posts from r/MachineLearning."""
              url = "https://www.reddit.com/r/MachineLearning/top.json"
              params = {"t": "day", "limit": limit}
              headers = {"User-Agent": "github-actions-heartbeat-ai-news/1.0"}
              try:
                  r = requests.get(url, params=params, headers=headers, timeout=10)
                  r.raise_for_status()
                  data = r.json()
              except Exception as e:
                  print(f"[WARN] Reddit fetch failed: {e}")
                  return []

              items = []
              for child in data.get("data", {}).get("children", []):
                  d = child.get("data", {})
                  title = d.get("title")
                  external_url = d.get("url")
                  permalink = d.get("permalink") or ""
                  url = external_url or ("https://www.reddit.com" + permalink)
                  if title and url:
                      items.append({
                          "title": title,
                          "url": url,
                          "source": "r/MachineLearning",
                      })
              return items

          # ---------- Markdown builder ----------

          def build_block(items, now_utc):
              lines = []
              lines.append(f"Last update: {now_utc:%Y-%m-%d %H:%M} UTC")
              lines.append("")
              if not items:
                  lines.append(
                      "- No fresh AI links found today. "
                      "*(Sources checked: Hacker News, r/MachineLearning)*"
                  )
              else:
                  for item in items:
                      title = item["title"].replace("\n", " ").strip()
                      url = item["url"]
                      source = item["source"]
                      lines.append(f"- [{title}]({url}) _(source: {source})_")
              return "\n".join(lines)

          # ---------- Main logic ----------

          def main():
              now = datetime.datetime.utcnow()

              try:
                  readme = README_PATH.read_text(encoding="utf-8")
              except FileNotFoundError:
                  raise SystemExit("README.md not found")

              hn_items = fetch_hn(5)
              reddit_items = fetch_reddit(5)

              # Merge + de-duplicate by URL, preserve order
              combined = []
              seen_urls = set()
              for item in hn_items + reddit_items:
                  key = item["url"]
                  if key in seen_urls:
                      continue
                  seen_urls.add(key)
                  combined.append(item)
                  if len(combined) >= 5:
                      break

              new_block = build_block(combined, now)

              pattern = re.compile(
                  re.escape(START) + r"[\s\S]*?" + re.escape(END),
                  re.M,
              )
              replacement = f"{START}\n{new_block}\n{END}"

              if pattern.search(readme):
                  updated = pattern.sub(replacement, readme)
              else:
                  # If markers were missing, append them at the end
                  updated = readme.rstrip() + "\n\n" + replacement + "\n"

              if updated != readme:
                  README_PATH.write_text(updated, encoding="utf-8")
                  print("README.md updated with fresh AI links.")
              else:
                  print("No changes to README; content is identical.")

          if __name__ == "__main__":
              main()
          PY

      - name: Commit and push
        env:
          GH_TOKEN: ${{ secrets.PAT_PUSH }}
        run: |
          set -euo pipefail

          git config user.name "Aviv Geron"
          git config user.email "geron.aviv@hotmail.com"

          if ! git diff --quiet; then
            git add README.md
            git commit -m "chore: daily AI multi-source news summary"
            git push "https://x-access-token:${GH_TOKEN}@github.com/${{github.repository}}" HEAD
          else
            echo "No changes to commit"
          fi
